{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLG561E Assignment 2.2: Regularizers\n",
    "\n",
    "In this notebook, you are going to implement regularization techniques widely used until recently in convolutional networks such as **Max Pooling** and **Dropout**\n",
    "\n",
    "Find `MaxPool2d`, `BatchNorm`, `Dropout` classes in **layer/layer.py** and complete the implementation of `forward` and `backward` methods for both of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from blg561e.layer.layer import MaxPool2d, Dropout, BatchNorm\n",
    "from blg561e.checks import rel_error, grad_check\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Dropout layer\n",
    "\n",
    "As we covered in the class, dropout is a well-known regularization technique for preventing overfitting of neural networks. What dropout does is basically zeroing out of some outputs of hidden layers at random. We recommend you to multiply the dropout factor with outputs in forward pass as it is done in common implementations. Recall that this is called **Inverted Dropout**.\n",
    "\n",
    "For more information on dropout, you can check the paper below.\n",
    "\n",
    "**Improving neural networks by preventing co-adaptation of feature detectors**, Hinton et al.\n",
    "https://arxiv.org/pdf/1207.0580.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout rate is:  0.3\n",
      "Percent of how much of input is zeroed out in training  0.699904\n",
      "Percent of how much of input is zeroed out in testing 0.0\n",
      "Dropout rate is:  0.5\n",
      "Percent of how much of input is zeroed out in training  0.500295\n",
      "Percent of how much of input is zeroed out in testing 0.0\n",
      "Dropout rate is:  0.8\n",
      "Percent of how much of input is zeroed out in training  0.199679\n",
      "Percent of how much of input is zeroed out in testing 0.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1773)\n",
    "\n",
    "x = np.random.randn(1000, 1000) + 1773\n",
    "for p in [0.3, 0.5, 0.8]:\n",
    "    dropout = Dropout(p=p)\n",
    "    dropout.mode = 'train'\n",
    "    out = dropout.forward(x)\n",
    "    dropout.mode = 'test'\n",
    "    out_test = dropout.forward(x)\n",
    "\n",
    "    print('Dropout rate is: ', p)\n",
    "    print('Percent of how much of input is zeroed out in training ', (out == 0).mean())\n",
    "    print('Percent of how much of input is zeroed out in testing', (out_test == 0).mean())\n",
    "\n",
    "# You can check wheter your implemention is true or not by looking at the percent of outputs set to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = Dropout(p=0.8)\n",
    "np.random.seed(1773)\n",
    "x = np.random.randn(10, 10) + 10\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "\n",
    "out = dropout.forward(x,seed=1773)\n",
    "dx = dropout.backward(dout)\n",
    "dx_num = grad_check(lambda xx: dropout.forward(xx, seed=1773), x, dout)\n",
    "\n",
    "print('Error on dx ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. MaxPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass\n",
    "Implement the forward pass for the max-pooling operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shape = (2, 3, 7, 7)\n",
    "x = np.linspace(-0.3, 0.3, num=np.prod(x_shape)).reshape(x_shape)\n",
    "maxPool = MaxPool2d(stride = 2, pool_width = 3, pool_height = 3)\n",
    "out = maxPool.forward(x)\n",
    "\n",
    "correct_out = np.array([[[[-0.26723549, -0.26313993, -0.25904437],\n",
    "   [-0.23856655, -0.23447099 ,-0.23037543],\n",
    "   [-0.20989761, -0.20580205, -0.20170648],],\n",
    "\n",
    "  [[-0.1668942,  -0.16279863, -0.15870307],\n",
    "   [-0.13822526, -0.13412969, -0.13003413],\n",
    "   [-0.10955631, -0.10546075, -0.10136519],],\n",
    "\n",
    "  [[-0.0665529,  -0.06245734, -0.05836177],\n",
    "   [-0.03788396, -0.0337884,  -0.02969283],\n",
    "   [-0.00921502, -0.00511945, -0.00102389],],],\n",
    "\n",
    "\n",
    " [[[ 0.0337884 ,  0.03788396,  0.04197952],\n",
    "   [ 0.06245734,  0.0665529,   0.07064846],\n",
    "   [ 0.09112628,  0.09522184,  0.09931741],],\n",
    "\n",
    "  [[ 0.13412969,  0.13822526,  0.14232082],\n",
    "   [ 0.16279863,  0.1668942,   0.17098976],\n",
    "   [ 0.19146758,  0.19556314,  0.1996587 ],],\n",
    "\n",
    "  [[ 0.23447099,  0.23856655,  0.24266212],\n",
    "   [ 0.26313993,  0.26723549,  0.27133106],\n",
    "   [ 0.29180887,  0.29590444,  0.3       ],],],])\n",
    "\n",
    "err = rel_error(out, correct_out)\n",
    "print('Error: ', rel_error(out, correct_out))\n",
    "assert err < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward pass\n",
    "Implement the backward pass for the max-pooling operation. You only need to pass the gradient from the maximum of the filter kernel position, the rest should be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1773)\n",
    "x = np.random.randn(10, 1, 8, 8)\n",
    "dout = np.random.randn(10, 1, 4, 4)\n",
    "max_pool = MaxPool2d(pool_height=2, pool_width=2, stride=2)\n",
    "dx_num = grad_check(lambda x: max_pool.forward(x), x, dout)\n",
    "\n",
    "out = max_pool.forward(x)\n",
    "dx = max_pool.backward(dout)\n",
    "\n",
    "# Your error should be around 1e-12\n",
    "print('Testing max_pool_backward_naive function:')\n",
    "print('dx error: ', rel_error(dx, dx_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward Pass**\n",
    "First read and understand the paper:\n",
    "\n",
    "S. Ioffe, C. Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
    "https://arxiv.org/pdf/1502.03167.pdf\n",
    "\n",
    "Implement the forward pass for the Batch Normalization technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should understand how the gamma and beta parameters affect to the output\n",
    "\n",
    "# An example of a single hidden layer with ReLU activation.\n",
    "np.random.seed(1773)\n",
    "N, D1, D2 = 200, 50, 3,\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "a = np.maximum(0, X.dot(W1))\n",
    "\n",
    "bn1 = BatchNorm(D2)\n",
    "\n",
    "print('Without using batchnorn')\n",
    "print('  mean of each feature/channel: ', a.mean(axis=0))\n",
    "print('  stds of each feature/channel ', a.std(axis=0))\n",
    "\n",
    "\n",
    "print(' Stats after batch normalization with gamma=1, beta=0')\n",
    "normalized = bn1.forward(a)\n",
    "print('  mean: ', normalized.mean(axis=0))\n",
    "print('  std: ', normalized.std(axis=0))\n",
    "\n",
    "\n",
    "bn1.gamma = np.array([1.0, 2.0, 3.0])\n",
    "bn1.beta = np.array([7, 3, 4])\n",
    "normalized  = bn1.forward(a)\n",
    "print(' Stats after batch normalization with arbitirary parameters')\n",
    "print('  mean: ', normalized.mean(axis=0))\n",
    "print('  std: ', normalized.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward Pass**\n",
    "Implement the forward pass for the Batch Normalization technique. Follow the paper by Ioffe et al. for the equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient check batchnorm backward pass\n",
    "np.random.seed(1773)\n",
    "N, D = 20, 6\n",
    "x = 3 * np.random.randn(N, D) + 13\n",
    "\n",
    "bn1 = BatchNorm(D)\n",
    "gamma = np.random.randn(D)\n",
    "beta = np.random.randn(D)\n",
    "dout = np.random.randn(N, D)\n",
    "\n",
    "fx = lambda x: bn1.forward(x, gamma=gamma, beta=beta)\n",
    "fg = lambda a: bn1.forward(x, gamma=a, beta=beta)\n",
    "fb = lambda b: bn1.forward(x, gamma=gamma, beta=b)\n",
    "\n",
    "dx_num = grad_check(fx, x, dout)\n",
    "da_num = grad_check(fg, gamma.copy(), dout)\n",
    "db_num = grad_check(fb, beta.copy(), dout)\n",
    "\n",
    "bn1.forward(x, gamma=gamma, beta=beta)\n",
    "dx, dgamma, dbeta = bn1.backward(dout)\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dgamma error: ', rel_error(da_num, dgamma))\n",
    "print('dbeta error: ', rel_error(db_num, dbeta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}